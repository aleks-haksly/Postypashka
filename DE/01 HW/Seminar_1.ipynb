{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZA51hxsORmE",
        "outputId": "982ffc85-1ab2-4cd4-8e59-7b38904183c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=1a80ce55c34ae875cc23059e5d80c5e3c120a417cbacc62222e2c816cd353722\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "Q3Cfbn2HOkAk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "7HznaphmwakL",
        "outputId": "f2977d98-d81c-4f83-a84d-73133edf26dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79bdbeb56ef0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://713051dcaece:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**RDD**"
      ],
      "metadata": {
        "id": "AAJjAlqsDu0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вспомогательные функции\n"
      ],
      "metadata": {
        "id": "_l3bTFHPkxqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_order = \"/content/drive/MyDrive/DE/LabRDD/dataset/order/order.csv\"\n",
        "path_to_customer = \"/content/drive/MyDrive/DE/LabRDD/dataset/customer/customer.csv\"\n",
        "path_to_product = \"/content/drive/MyDrive/DE/LabRDD/dataset/product/product.csv\"\n",
        "\n",
        "def createRDD(spark:SparkSession, path):\n",
        "  return spark.sparkContext.textFile(path)\\\n",
        "                    .map(lambda x: x.split('\\t'))"
      ],
      "metadata": {
        "id": "0gcVpRsKk2aF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  # Задача 1\n",
        "  * 1. Загрузить в RDD файл src/test/resources/input/order\n",
        "  * 2. Выбрать строки в которых поле статус заказа delivered\n",
        "  * 3. Выбрать ключ (customerID), значение (numberOfProduct, 1)\n",
        "  * 4. Написать функцию reduce для расчёта\n",
        "  * 5. Применит к RDD из п.3 метод reduceByKey c функцией из п.4\n",
        "  * 6. Вывести или записать результат"
      ],
      "metadata": {
        "id": "BcACIvWiYggX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders = spark.sparkContext.textFile(path_to_order)\n",
        "orders = orders.map(lambda x: x.split('\\t'))\\\n",
        "              .filter(lambda x: 'delivered' in x[5])\\\n",
        "              .map(lambda x: (x[0], (x[3],1)))\\\n",
        "              .reduceByKey(lambda x,y: ((int(x[0])+int(y[0])),(x[1]+y[1]))).take(5)\n",
        "print(orders)\n"
      ],
      "metadata": {
        "id": "uHHHEkYhSnAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae976b4-57ae-4b83-d119-b78b05e21d4e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('1', (3000, 9)), ('4', (1300, 8)), ('2', (1250, 11)), ('3', ('100', 1)), ('5', (1300, 8))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 2\n",
        "   * 1. Загрузить в RDD файл src/test/resources/input/order\n",
        "   * 2. Распарсить строки RDD\n",
        "   * 3. Выбрать ключ поле (customerID), в значение (orderDate, numberOfProduct, productID)\n",
        "   * 4. Загрузить в RDD файл customer\n",
        "   * 5. Распарсить строки RDD\n",
        "   * 6. Выбрать ключ поле (id), в значение (name)\n",
        "   * 7. Выполнить внутреннее соединение RDD из п.6 и п.9\n",
        "   * 8. Выбрать ключ (productID), в значение (customer.name, orderDate,  numberOfProduct)\n",
        "   * 9. Загрузить в RDD файл product.csv\n",
        "   * 10. Распарсить строки RDD\n",
        "   * 11. Выбрать ключ (id), значение (price)\n",
        "   * 12. Выполнить внутреннее соединение с RDD из п.11 и п.14\n",
        "   * 13. Выбрать ключ (customer.name, order.orderDate), значение (order.numberOfProduct * product.price)\n",
        "   * 14. Расчитать сумму в значении по ключу\n",
        "   * 15. Вывести результат"
      ],
      "metadata": {
        "id": "ioq_0k79XVIX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smNWeMqmS6D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1-sB4PGdoDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHB531_FjTKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 3\n",
        " * 1. Загрузить в RDD файл src/test/resources/input/product\n",
        " * 2. Распарсить строки в RDD\n",
        " * 3. Выбрать ключ поле id, в значение name\n",
        " * 4. Загрузить в RDD файд src/test/resources/input/order\n",
        " * 5. Распарсить строки в RDD\n",
        " * 6. Выбрать ключ поле productID, в значение numberOfProduct\n",
        " * 7. Посчитать кол-во проданных продуктов\n",
        " * 8. Выполнить левое соединение двух RDD\n",
        " * 9. Выполнить фильтрацию и оставить только те строки где значение numberOfProducts 0 или None\n",
        " * 10. Вывести результат или записать в директорию"
      ],
      "metadata": {
        "id": "45RHgDpy37fv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u6Gmt7NjqmV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YVCo_Zm4WFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 4\n",
        " * 1. Загрузить в RDD файд src/test/resources/input/order.csv\n",
        " * 2. Распарсить строки в RDD\n",
        " * 3. Выбрать только те транзакции у которых статус delivered\n",
        " * 4. Выбрать ключ (customerID), значение (numberOfProducts)\n",
        " * 5. Выполнить группировку по ключу\n",
        " * 6. Посчитать сумму по значению и разделить на размер коллекции\n",
        " * 7. Вывести результат или записать в директорию"
      ],
      "metadata": {
        "id": "ke2CWIlEID37"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TcNB5yIUIDSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATAFRAME**"
      ],
      "metadata": {
        "id": "ojob0rIRD7aU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вспомогательные функции"
      ],
      "metadata": {
        "id": "RimZITiTdfgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "class Parameters:\n",
        "  path_laptop = \"/content/drive/MyDrive/DE/LabDF/dataset/laptop/laptop.txt\"\n",
        "  path_pc = \"/content/drive/MyDrive/DE/LabDF/dataset/pc/pc.txt\"\n",
        "  path_printer = \"/content/drive/MyDrive/DE/LabDF/dataset/printer/printer.txt\"\n",
        "  path_product = \"/content/drive/MyDrive/DE/LabDF/dataset/product/product.txt\"\n",
        "\n",
        "  table_laptop = \"laptop\"\n",
        "  table_pc = \"pc\"\n",
        "  table_printer = \"printer\"\n",
        "  table_product = \"product\"\n",
        "\n",
        "  def createTable(self, name:str, spark:SparkSession, structType: StructType, path:str, delimiter: str = ','):\n",
        "      spark.read\\\n",
        "            .options(delimiter = delimiter, nullValue = \"\\\\N\")\\\n",
        "            .schema(structType)\\\n",
        "            .csv(path)\\\n",
        "            .createOrReplaceTempView(name)\n",
        "  def initTables(self, spark:SparkSession):\n",
        "    self.createTable(Parameters.table_pc, spark, PC.structType, Parameters.path_pc)\n",
        "    self.createTable(Parameters.table_product, spark, Product.structType, Parameters.path_product)"
      ],
      "metadata": {
        "id": "Xn9zKwS0K6rL"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PC:\n",
        "  structType = StructType([\n",
        "      StructField(\"ID\", IntegerType()),\n",
        "      StructField(\"MODEL\", IntegerType()),\n",
        "      StructField(\"SPEED\", IntegerType()),\n",
        "      StructField(\"RAM\", IntegerType()),\n",
        "      StructField(\"C2\", IntegerType()),\n",
        "      StructField(\"C3\", StringType()),\n",
        "      StructField(\"PRICE\", IntegerType()),\n",
        "  ])\n",
        "class Product:\n",
        "  structType = StructType([\n",
        "      StructField(\"Maker\", StringType()),\n",
        "      StructField(\"Model\", StringType()),\n",
        "      StructField(\"Type\", StringType())\n",
        "  ])"
      ],
      "metadata": {
        "id": "8orCH6v_TaX6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = Parameters()\n",
        "p.initTables(spark)"
      ],
      "metadata": {
        "id": "TxRB6jrHUGN2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 1\n",
        "Для каждого значения скорости ПК, превышающего 600 МГц, определите среднюю цену ПК с такой же скоростью.\n",
        "Вывести: speed, средняя цена.\n",
        "\n",
        "SELECT pc.speed, AVG(pc.price)\n",
        "FROM pc\n",
        "WHERE pc.speed > 600\n",
        "GROUP BY pc.speed\n",
        ")"
      ],
      "metadata": {
        "id": "OQWCmmxMdo0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, avg\n",
        "pc = spark.read.table(\"pc\")\\\n",
        "              .select(\"speed\",\"price\")\\\n",
        "              .filter(col(\"price\") > 600)\\\n",
        "              .groupBy(\"speed\")\\\n",
        "              .agg(avg(col(\"price\")).alias(\"avg_price\"))\\\n",
        "              .select(\"speed\",\"avg_price\").show()"
      ],
      "metadata": {
        "id": "kTTcefXbdobG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04688a92-06e2-40dd-8e71-de91c4a36898"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "|speed|avg_price|\n",
            "+-----+---------+\n",
            "|  900|    980.0|\n",
            "|  800|    970.0|\n",
            "|  750|    900.0|\n",
            "|  600|    850.0|\n",
            "+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 2\n",
        "Вывести все строки из таблицы Product, кроме трех строк с наименьшими номерами моделей и трех строк с наибольшими номерами моделей.\n",
        "\n",
        "  Select maker, model, type from\n",
        "  (\n",
        "  Select\n",
        "  row_number() over (order by model) p1,\n",
        "  row_number() over (order by model DESC) p2,\n",
        "  from Product\n",
        "  ) t1\n",
        "  where p1 > 3 and p2 > 3\n"
      ],
      "metadata": {
        "id": "tNLA4oxPfL-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, desc\n",
        "\n",
        "product = spark.table(\"product\")\n",
        "MaxVal = Window.orderBy(\"model\")\n",
        "product1 = product.withColumn(\"p1\", row_number().over(MaxVal))\n",
        "MinVal = Window.orderBy(desc(\"model\"))\n",
        "product1.withColumn(\"p2\", row_number().over(MinVal))\\\n",
        "        .select(\"maker\", \"model\", \"type\")\\\n",
        "        .filter((col(\"p1\")>3) & (col(\"p2\")>3)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdHwvInSVnve",
        "outputId": "7a7f6d36-505b-49e0-c85b-ba37174ccf8b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+\n",
            "|maker|model|   type|\n",
            "+-----+-----+-------+\n",
            "|    B| 1750| Laptop|\n",
            "|    E| 1434|Printer|\n",
            "|    D| 1433|Printer|\n",
            "|    A| 1408|Printer|\n",
            "|    A| 1401|Printer|\n",
            "|    C| 1321| Laptop|\n",
            "|    A| 1298| Laptop|\n",
            "|    D| 1288|Printer|\n",
            "|    A| 1276|Printer|\n",
            "|    E| 1260|     PC|\n",
            "+-----+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 3\n",
        "  Найти тех производителей ПК, все модели ПК которых имеются в таблице PC.\n",
        "\n",
        "  SELECT p.maker\n",
        "  FROM product p\n",
        "  LEFT JOIN pc ON pc.model = p.model\n",
        "  WHERE p.type = 'PC'\n",
        "  GROUP BY p.maker\n",
        "  HAVING COUNT(p.model) = COUNT(pc.model)\n",
        "  "
      ],
      "metadata": {
        "id": "ZBftHNDgb6lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFNbFTsAcCPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задача 4\n",
        "Найдите производителей принтеров, которые производят ПК с наименьшим объемом RAM и с самым быстрым процессором среди всех ПК, имеющих наименьший объем RAM. Вывести: Maker\n",
        "\n",
        "  SELECT DISTINCT maker\n",
        "  FROM product\n",
        "   WHERE model IN (\n",
        "  SELECT model\n",
        "  FROM pc\n",
        "  WHERE ram = (\n",
        "    SELECT MIN(ram)\n",
        "    FROM pc\n",
        "    )\n",
        "  AND speed = (\n",
        "    SELECT MAX(speed)\n",
        "    FROM pc\n",
        "    WHERE ram = (\n",
        "     SELECT MIN(ram)\n",
        "     FROM pc\n",
        "     )\n",
        "    )\n",
        "  )\n",
        "  AND\n",
        "  maker IN (\n",
        "  SELECT maker\n",
        "  FROM product\n",
        "  WHERE type='printer'\n",
        "  )"
      ],
      "metadata": {
        "id": "8yyjsu7qiaw_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phUlVWmShfdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cfccac-215e-49cf-e961-bbdeeb60e430"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+\n",
            "|Maker|Model|   Type|\n",
            "+-----+-----+-------+\n",
            "|    B| 1121|     PC|\n",
            "|    A| 1232|     PC|\n",
            "|    A| 1233|     PC|\n",
            "|    E| 1260|     PC|\n",
            "|    A| 1276|Printer|\n",
            "|    D| 1288|Printer|\n",
            "|    A| 1298| Laptop|\n",
            "|    C| 1321| Laptop|\n",
            "|    A| 1401|Printer|\n",
            "|    A| 1408|Printer|\n",
            "|    D| 1433|Printer|\n",
            "|    E| 1434|Printer|\n",
            "|    B| 1750| Laptop|\n",
            "|    A| 1752| Laptop|\n",
            "|    E| 2113|     PC|\n",
            "|    E| 2112|     PC|\n",
            "+-----+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RBFmqeejyZ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}